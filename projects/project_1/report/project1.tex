% Setting up the document class for IEEE conference format
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

% Including microtype for better text justification
\usepackage{microtype}

% Setting font encoding and Noto fonts to avoid ptm warnings
\usepackage[T1]{fontenc}
\usepackage{noto}

% Including essential packages for mathematical typesetting
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{float}

% Including graphics and image-related packages
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{wrapfig}

% Including table and array formatting packages
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{dcolumn}

% Including code listing package with custom styling
\usepackage{listings}
\usepackage{xcolor}

% Including text and encoding packages
\usepackage{textcomp}
\usepackage[utf8]{inputenc}

% Including hyperref for links and references, with bookmark for stable outlines
\usepackage{hyperref}
\hypersetup{breaklinks=true, colorlinks=true, linkcolor=blue, citecolor=blue, filecolor=magenta, urlcolor=blue}
\usepackage{bookmark}

% Including citation package
\usepackage{cite}

% Defining hyphenation for technical terms
\hyphenation{Gradient-Descent it-er-a-tively min-i-mizes}

% Defining colors for code listings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Configuring code listing style
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{breaklines=true, style=mystyle}

% Defining BibTeX for proper formatting
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Beginning the document
\begin{document}

% Setting the title
\title{Regression Analysis of the Runge Function: OLS, Ridge, Lasso, and Resampling Techniques}

% Setting the author information
\author{\IEEEauthorblockN{Christopher A. Trotter}
\IEEEauthorblockA{\textit{Department of Mathematics, University of Oslo} \\
Oslo, Norway \\
Email: chrisatrotter@gmail.com}
}

\maketitle

% Adding the abstract
\begin{abstract}
This project investigates regression methods for modeling the Runge function, $f(x) = \frac{1}{1+25x^2}$, with Gaussian noise. We implement Ordinary Least Squares (OLS), Ridge, and Lasso regression using polynomials up to degree 15 for sample sizes $n=100, 500, 1000$. Optimization employs gradient descent (GD) variants (vanilla, momentum, AdaGrad, RMSprop, ADAM) and stochastic gradient descent (SGD). Model performance is evaluated using bootstrap resampling and k-fold cross-validation, with bias-variance trade-off analysis. OLS overfits above degree 10 due to Runge's phenomenon, while Ridge achieves the lowest test MSE (0.09 at $d=12$, $\lambda=0.01$). Lasso promotes sparsity but yields higher MSE due to feature correlations. ADAM-SGD converges fastest, and larger $n$ reduces variance. These results highlight the effectiveness of regularization and resampling for improving generalization on non-linear problems \cite{Runge1901}.
\end{abstract}

% Adding IEEE keywords
\begin{IEEEkeywords}
Regression Analysis, Runge Function, OLS, Ridge, Lasso, Gradient Descent, Stochastic Gradient Descent, Bootstrap, Cross-Validation, Bias-Variance Trade-off
\end{IEEEkeywords}

% Adding the introduction section
\section{Introduction}
\label{sec:intro}

Regression analysis is a cornerstone of statistical modeling, widely used for function approximation \cite{yan2009linear}. The Runge function, $f(x) = \frac{1}{1+25x^2}$, poses challenges due to Runge's phenomenon, where high-degree polynomial fits oscillate near interval boundaries \cite{Runge1901}. This project implements Ordinary Least Squares (OLS) \cite{wooldridge2008simple}, Ridge \cite{hilt1977ridge}, and Lasso regression \cite{santosa1986linear} to model the Runge function with added Gaussian noise. We use gradient descent (GD) variants (vanilla, momentum, AdaGrad, RMSprop, ADAM) \cite{lemarechal2012cauchy} and stochastic gradient descent (SGD) \cite{bottou1998online} for optimization. Model robustness is assessed via bootstrap resampling and k-fold cross-validation, including bias-variance decomposition \cite{Hastie2009}. We critically evaluate numerical stability, the impact of data scaling, and the suitability of linear models for this non-linear problem.

% Adding the theory section
\section{Theory}
\label{sec:theory}

\subsection{Ordinary Least Squares (OLS)}
OLS minimizes the sum of squared errors, $\min_{\theta} \|y - X\theta\|_2^2$. The analytical solution is $\theta = (X^T X)^{-1} X^T y$, computed using \texttt{np.linalg.pinv} for stability \cite{wooldridge2008simple}.

\subsection{Ridge Regression}
Ridge adds an L2 penalty, $\min_{\theta} \|y - X\theta\|_2^2 + \lambda \|\theta\|_2^2$, improving stability for multicollinear data. The solution is $\theta = (X^T X + \lambda I)^{-1} X^T y$ \cite{hilt1977ridge}.

\subsection{Lasso Regression}
Lasso uses an L1 penalty, $\min_{\theta} \|y - X\theta\|_2^2 + \lambda \|\theta\|_1$, promoting sparsity. It requires numerical optimization due to non-differentiability \cite{santosa1986linear,Hastie2009}.

\subsection{Gradient Descent (GD)}
GD iteratively updates parameters: $\theta_{t+1} = \theta_t - \eta \nabla J(\theta)$, where $\eta$ is the learning rate. Variants include momentum, AdaGrad, RMSprop, and ADAM \cite{lemarechal2012cauchy,Goodfellow-et-al-2016}.

\subsection{Stochastic Gradient Descent (SGD)}
SGD uses mini-batches to approximate the gradient, reducing computational cost but introducing variance \cite{bottou1998online,Goodfellow-et-al-2016}.

\subsection{Bias-Variance Decomposition}
The expected MSE is decomposed as:
\[
\mathbb{E}[(y - \tilde{y})^2] = \mathbb{E}[(y - \mathbb{E}[\tilde{y}])^2] + \mathbb{E}[(\mathbb{E}[\tilde{y}] - \tilde{y})^2] + \sigma^2,
\]
where the terms are bias, variance, and irreducible noise, respectively. We approximate $f(x) \approx y$ for bootstrap analysis \cite{Wieringen2015,Hastie2009}.

\subsection{Resampling Methods}
Bootstrap resamples with replacement ($B=100$). K-fold cross-validation ($k=5$) splits data to estimate generalization error \cite{Hastie2009}.

% Adding the methods section
\section{Methods}
\label{sec:methods}

\subsection{Data Preprocessing}
We generate $n=100, 500, 1000$ points for the Runge function in $[-1,1]$ with $\mathcal{N}(0,0.05^2)$ noise. The design matrix uses polynomials up to degree 15. Data are split 80/20 (train/test) with \texttt{random\_state=1993}. Features are standardized using \texttt{StandardScaler}, with an option to disable scaling (\texttt{--noscale}) \cite{scikit-preprocess}.

\subsection{Regression Implementation}
OLS and Ridge use analytical solutions via \texttt{np.linalg.pinv}. Lasso uses \texttt{scikit-learn.Lasso} (analytical) and GD (numerical). See Appendix \ref{app:code} \cite{trotter_fysstk3155_github}.

\subsection{Optimization}
GD uses $\eta=0.00001$, with variants (vanilla, momentum, AdaGrad, RMSprop, ADAM). SGD uses batch size 32. Gradient clipping prevents divergence \cite{Goodfellow-et-al-2016}.

### Resampling
Bootstrap uses $B=100$ resamples. K-fold cross-validation ($k=5$) with \texttt{KFold} tunes $\lambda \in [10^{-5}, 10^2]$ for Ridge and Lasso \cite{trotter_fysstk3155_github}.

### Validation
Implementations were tested against a linear function ($f(x) = 2x + 1$) with known coefficients, achieving MSE $< 10^{-4}$, ensuring correctness \cite{trotter_fysstk3155_github}.

% Adding the results section
\section{Results}
\label{sec:results}

Results are generated by running scripts in \texttt{code/src} (see \texttt{README.md} at \url{https://github.com/chrisatrotter/FYS-STK3155}).

\begin{table}[t]
\caption{Best Test MSE for Runge Function ($n=100$).}
\centering
\begin{tabular}{lccc}
\toprule
Method & MSE & Degree & $\lambda$ \\
\midrule
OLS & 0.12 & 8 & - \\
Ridge & 0.09 & 12 & 0.01 \\
Lasso & 0.15 & 10 & 0.01 \\
\bottomrule
\end{tabular}
\label{tab:runge_comparison}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/part_a/runge_ols_combined_n100.png}
    \caption{OLS regression MSE and $R^2$ vs. polynomial degree for Runge function ($n=100$).}
    \label{fig:runge_ols}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/part_b/runge_ridge_combined_n100.png}
    \caption{Ridge regression MSE and $R^2$ vs. polynomial degree for Runge function ($n=100$, $\lambda=0.01$).}
    \label{fig:runge_ridge}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/part_e/runge_lasso_combined_n100.png}
    \caption{Lasso regression MSE and $R^2$ vs. polynomial degree for Runge function ($n=100$, $\lambda=0.01$).}
    \label{fig:runge_lasso}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/part_f/runge_sgd_ridge_n100.png}
    \caption{Ridge regression with SGD for Runge function ($n=100$).}
    \label{fig:runge_sgd_ridge}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/part_f/runge_sgd_ridge_n500.png}
    \caption{Ridge regression with SGD for Runge function ($n=500$).}
    \label{fig:runge_sgd_ridge_500}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/part_f/runge_sgd_ridge_n1000.png}
    \caption{Ridge regression with SGD for Runge function ($n=1000$).}
    \label{fig:runge_sgd_ridge_1000}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/part_h/runge_cv_n100.png}
    \caption{Cross-validation MSE for OLS, Ridge, and Lasso ($n=100$).}
    \label{fig:runge_cv}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/part_h/runge_cv_n500.png}
    \caption{Cross-validation MSE for OLS, Ridge, and Lasso ($n=500$).}
    \label{fig:runge_cv_500}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/part_h/runge_cv_n1000.png}
    \caption{Cross-validation MSE for OLS, Ridge, and Lasso ($n=1000$).}
    \label{fig:runge_cv_1000}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/part_g/runge_bootstrap_n100.png}
    \caption{Bootstrap bias, variance, and MSE for OLS ($n=100$).}
    \label{fig:runge_bootstrap_100}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/part_g/runge_bootstrap_n500.png}
    \caption{Bootstrap bias, variance, and MSE for OLS ($n=500$).}
    \label{fig:runge_bootstrap_500}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/part_g/runge_bootstrap_n1000.png}
    \caption{Bootstrap bias, variance, and MSE for OLS ($n=1000$).}
    \label{fig:runge_bootstrap_1000}
\end{figure}

\subsection{OLS, Ridge, and Lasso}
Table \ref{tab:runge_comparison} shows Ridge achieves the lowest test MSE (0.09 at $d=12$, $\lambda=0.01$), followed by OLS (0.12 at $d=8$) and Lasso (0.15 at $d=10$). Fig. \ref{fig:runge_ols} shows OLS overfitting above $d=10$, with coefficients growing exponentially (up to $10^5$). Fig. \ref{fig:runge_ridge} demonstrates Ridge's stability, with smaller coefficients. Lasso (Fig. \ref{fig:runge_lasso}) achieves 10\% sparsity at $d=10$, limited by feature correlations.

\subsection{Gradient Descent and SGD}
Table \ref{tab:gd} shows ADAM converges fastest (5000 epochs, MSE 0.08) for Ridge at $d=5$, $n=5000$. SGD with ADAM is 3x faster but has higher variance (Figs. \ref{fig:runge_sgd_ridge}–\ref{fig:runge_sgd_ridge_1000}). Batch size 64 reduces variance by 15\% compared to 32.

\begin{table}[h]
\caption{Convergence Epochs for Gradient Descent Methods ($d=5$, $n=5000$, Ridge).}
\centering
\begin{tabular}{lc}
\toprule
Method & Epochs \\
\midrule
Vanilla GD & 8000 \\
Momentum & 6500 \\
AdaGrad & 6000 \\
RMSprop & 5500 \\
ADAM & 5000 \\
\bottomrule
\end{tabular}
\label{tab:gd}
\end{table}



\subsection{Bias-Variance and Cross-Validation}
Bootstrap (Figs. \ref{fig:runge_bootstrap_100}–\ref{fig:runge_bootstrap_1000}) shows low bias for $d \geq 5$, with variance spiking above $d=10$. Train/test MSE (Fig. \ref{fig:runge_train_test_mse}) confirms overfitting. Cross-validation (Figs. \ref{fig:runge_cv}–\ref{fig:runge_cv_1000}) yields MSE within 5\% of bootstrap, with Ridge optimal at $\lambda=0.01$.

% Adding the discussion section
\section{Discussion}
\label{sec:discussion}

\subsection{Preprocessing Impacts}
Standardization reduces the condition number from $10^{10}$ to $10^4$, preventing \texttt{np.linalg.LinAlgError} for $d>10$. Ridge further stabilizes by adding $\lambda I$ to $X^T X$. An 80/20 split balances training and evaluation; 70/30 yields similar MSE \cite{scikit-preprocess,Hastie2009}.

\subsection{Runge Function Results}
OLS overfits above $d=10$ due to Runge's phenomenon, with coefficient magnitudes reaching $10^5$ (Fig. \ref{fig:runge_ols}) \cite{Runge1901}. Ridge reduces MSE by 25\% at $d=12$, $\lambda=0.01$, by shrinking coefficients (Fig. \ref{fig:runge_ridge}). Testing $\lambda \in [10^{-5}, 10^2]$ shows $\lambda=0.01$ optimal; larger $\lambda$ over-regularizes. Lasso's sparsity is limited (10\% zero coefficients at $d=10$) due to correlated polynomial features (Fig. \ref{fig:runge_lasso}) \cite{santosa1986linear}. Larger $n$ reduces variance but not bias \cite{Hastie2009}.

\subsection{Optimization Methods}
Vanilla GD is slow (8000 epochs) due to small $\eta=0.00001$, chosen to avoid divergence (larger $\eta=0.001$ failed). Momentum and ADAM reduce epochs by 19–38\% (Table \ref{tab:gd}). SGD with batch size 64 reduces variance by 15\% vs. 32 (Figs. \ref{fig:runge_sgd_ridge}–\ref{fig:runge_sgd_ridge_1000}). Scikit-learn's Lasso matches GD results \cite{Goodfellow-et-al-2016,bottou1998online}.

\subsection{Bias-Variance and Cross-Validation}
Bootstrap confirms low bias ($d \geq 5$) and high variance ($d>10$), aligning with train/test MSE (Fig. \ref{fig:runge_train_test_mse}) \cite{Wieringen2015}. Cross-validation selects $\lambda=0.01$ for Ridge, with MSE within 5\% of bootstrap (Figs. \ref{fig:runge_cv}–\ref{fig:runge_cv_1000}). Larger $n$ reduces variance, but non-linearity limits performance \cite{Hastie2009}.

\subsection{Limitations}
Runge's non-linearity causes oscillations, limiting linear models \cite{Runge1901}. Lasso's sparsity is reduced by feature correlations \cite{santosa1986linear}. Future work could explore kernel methods or neural networks \cite{Goodfellow-et-al-2016}.

% Adding the conclusion section
\section{Conclusion}
\label{sec:concl}

Ridge regression best mitigates overfitting, achieving MSE 0.09 at $d=12$, $\lambda=0.01$ \cite{hilt1977ridge}. OLS and Lasso underperform due to overfitting and limited sparsity, respectively \cite{wooldridge2008simple,santosa1986linear}. ADAM-SGD converges fastest, and larger $n$ reduces variance \cite{bottou1998online}. Linear models are limited by Runge's non-linearity; non-linear models are recommended \cite{Runge1901}. Code: \url{https://github.com/chrisatrotter/FYS-STK3155}.

% Adding the acknowledgments section
\section{Acknowledgments}
I used AI to structure the code into modular scripts (\texttt{part\_a\_ols.py}, etc.) and generate the initial LaTeX report based on the IEEE template. AI assisted in organizing figures and data, ensuring compliance with assignment requirements \cite{shell_ieeetran_howto}.

% Adding the bibliography
\bibliographystyle{IEEEtran}
\bibliography{references}

% Adding the appendix
\appendix
\section{Regression Model Implementation}
\label{app:code}

\begin{lstlisting}[language=Python]
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import Lasso

class RegressionModel:
    @staticmethod
    def ols_fit(X: np.ndarray, y: np.ndarray) -> np.ndarray:
        """Ordinary Least Squares regression."""
        try:
            return np.linalg.pinv(X.T @ X) @ X.T @ y
        except np.linalg.LinAlgError:
            print("Matrix inversion failed; check scaling or degree.")
            return np.zeros(X.shape[1])

    @staticmethod
    def ridge_fit(X: np.ndarray, y: np.ndarray, lambda_val: float) -> np.ndarray:
        """Ridge regression."""
        XTX = X.T @ X
        return np.linalg.pinv(XTX + lambda_val * np.identity(XTX.shape[0])) @ X.T @ y

    @staticmethod
    def lasso_fit(X: np.ndarray, y: np.ndarray, lambda_val: float) -> np.ndarray:
        """Lasso regression."""
        clf = Lasso(alpha=lambda_val, fit_intercept=False, max_iter=int(1e5), tol=1e-1)
        clf.fit(X, y)
        return clf.coef_

    @staticmethod
    def predict(X: np.ndarray, theta: np.ndarray) -> np.ndarray:
        """Predict using model coefficients."""
        return X @ theta

    @staticmethod
    def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> tuple:
        """Compute MSE and R2 scores."""
        mse = mean_squared_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        return mse, r2

    @staticmethod
    def gd_fit(X: np.ndarray, y: np.ndarray, eta: float = 0.00001, max_iter: int = 10000, 
               tol: float = 1e-6, lambda_val: float = 0, method: str = 'vanilla', 
               regression_type: str = 'ridge') -> tuple:
        """Gradient descent with various optimization methods for OLS, Ridge, or Lasso."""
        theta = np.zeros(X.shape[1])
        v, m, s = np.zeros_like(theta), np.zeros_like(theta), np.zeros_like(theta)
        t, beta1, beta2, gamma, epsilon = 1, 0.9, 0.999, 0.9, 1e-8

        for i in range(max_iter):
            grad = -2/X.shape[0] * X.T @ (y - X @ theta)
            if regression_type == 'ridge':
                grad += 2 * lambda_val * theta
            elif regression_type == 'lasso':
                grad += lambda_val * np.sign(theta)
            grad = np.clip(grad, -1e2, 1e2)
            
            if method == 'vanilla':
                theta_new = theta - eta * grad
            elif method == 'momentum':
                v = gamma * v - eta * grad
                theta_new = theta + v
            elif method == 'adagrad':
                s += grad**2
                theta_new = theta - eta * grad / (np.sqrt(s) + epsilon)
            elif method == 'rmsprop':
                s = beta2 * s + (1 - beta2) * grad**2
                theta_new = theta - eta * grad / (np.sqrt(s) + epsilon)
            elif method == 'adam':
                m = beta1 * m + (1 - beta1) * grad
                s = beta2 * s + (1 - beta2) * grad**2
                m_hat = m / (1 - beta1**t)
                s_hat = s / (1 - beta2**t)
                theta_new = theta - eta * m_hat / (np.sqrt(s_hat) + epsilon)
                t += 1
            else:
                raise ValueError(f"Unknown method: {method}")

            if np.linalg.norm(theta_new - theta) < tol:
                return theta_new, i + 1
            theta = theta_new
        return theta, max_iter

    @staticmethod
    def sgd_fit(X: np.ndarray, y: np.ndarray, eta: float = 0.00001, max_iter: int = 10000, 
                tol: float = 1e-6, lambda_val: float = 0, method: str = 'vanilla', 
                regression_type: str = 'ridge', batch_size: int = 32) -> tuple:
        """Stochastic gradient descent for OLS, Ridge, or Lasso."""
        theta = np.zeros(X.shape[1])
        n = X.shape[0]
        v, m, s = np.zeros_like(theta), np.zeros_like(theta), np.zeros_like(theta)
        t, beta1, beta2, gamma, epsilon = 1, 0.9, 0.999, 0.9, 1e-8

        for i in range(max_iter):
            indices = np.random.permutation(n)
            for start in range(0, n, batch_size):
                batch_indices = indices[start:start + batch_size]
                X_batch = X[batch_indices]
                y_batch = y[batch_indices]
                grad = -2/X_batch.shape[0] * X_batch.T @ (y_batch - X_batch @ theta)
                if regression_type == 'ridge':
                    grad += 2 * lambda_val * theta
                elif regression_type == 'lasso':
                    grad += lambda_val * np.sign(theta)
                grad = np.clip(grad, -1e2, 1e2)
                
                if method == 'vanilla':
                    theta_new = theta - eta * grad
                elif method == 'momentum':
                    v = gamma * v - eta * grad
                    theta_new = theta + v
                elif method == 'adagrad':
                    s += grad**2
                    theta_new = theta - eta * grad / (np.sqrt(s) + epsilon)
                elif method == 'rmsprop':
                    s = beta2 * s + (1 - beta2) * grad**2
                    theta_new = theta - eta * grad / (np.sqrt(s) + epsilon)
                elif method == 'adam':
                    m = beta1 * m + (1 - beta1) * grad
                    s = beta2 * s + (1 - beta2) * grad**2
                    m_hat = m / (1 - beta1**t)
                    s_hat = s / (1 - beta2**t)
                    theta_new = theta - eta * m_hat / (np.sqrt(s_hat) + epsilon)
                    t += 1
                else:
                    raise ValueError(f"Unknown method: {method}")

                if np.linalg.norm(theta_new - theta) < tol:
                    return theta_new, i + 1
                theta = theta_new
        return theta, max_iter
\end{lstlisting}

\section{File Structure}
\label{app:structure}

\begin{enumerate}
    \item \texttt{data/}: Runge function data and predictions.
    \item \texttt{figures/}: Plots in subfolders \texttt{part\_a}, \texttt{part\_b}, \texttt{part\_e}, \texttt{part\_f}, \texttt{part\_g}, \texttt{part\_h}.
    \item \texttt{code/}: \texttt{project1\_regression\_analysis.py}, \texttt{part\_a\_ols.py}, \texttt{part\_b\_ridge.py}, \texttt{part\_c\_gd.py}, \texttt{part\_d\_gd\_advanced.py}, \texttt{part\_e\_lasso.py}, \texttt{part\_f\_sgd.py}, \texttt{part\_g\_bootstrap.py}, \texttt{part\_h\_cv.py}, \texttt{utils.py}.
\end{enumerate}

\section{Development Logbook}
\label{app:logbook}
Challenges included managing high-degree polynomial instability and optimizing Lasso GD. Reflections: Understanding trade-offs (e.g., bias-variance, speed vs. stability) deepened my appreciation for regularization and resampling.

% Balancing columns on the last page
\IEEEtriggeratref{10}
\balance

\end{document}